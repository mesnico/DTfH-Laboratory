{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "scFQtUonWXjR",
        "X94EcR9EvBYW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesnico/DTfH-Laboratory/blob/main/2025/text_to_image_similarity_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4MK2n10BGmc"
      },
      "source": [
        "# Laboratory on Text and Image Representations for Text-to-Image Similarity Search\n",
        "-----------------\n",
        "\n",
        "You'll learn to:\n",
        "\n",
        "*   Represent images using features extracted from multimodal deep neural networks.\n",
        "*   Search images using textual descriptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0IgnESPfmK1"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "First of all, we need to download and unzip the image dataset, and install some Python dependencies.\n",
        "\n",
        "We will use MIRFlickr5k, a subset of the larger [MIRFlickr25k](https://press.liacs.nl/mirflickr/mirdownload.html), which contains photographs downloaded from the popular Flickr website.\n",
        "\n",
        "Run the following for getting the environment ready! This could take some minutes...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6_FRKubksmR"
      },
      "source": [
        "import os\n",
        "if not os.path.exists('mirflickr5k'):\n",
        "  # !wget mb-messina.isti.cnr.it/mirflickr5k.zip\n",
        "  !gdown --id 1sEBg-sZgSQac0W7fyPecj-s_uWAjLLh9\n",
        "  !unzip -n mirflickr5k.zip\n",
        "else:\n",
        "  print('Dataset already downloaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we install some python dependency and we import the needed python packages"
      ],
      "metadata": {
        "id": "dah8Ta-nRIWE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxr0appUBuAh"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import transformers\n",
        "from PIL import Image\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "import requests\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "random.seed(42)\n",
        "\n",
        "euclidean_distance = torch.cdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the function for computing the k-NN from the distances\n",
        "def k_nearest_neighbors(distances, k=5):\n",
        "  nq, ndb = distances.shape\n",
        "\n",
        "  sorted_distances = distances.argsort(axis=1)  # sort the scores ascending, for each query\n",
        "  topk = sorted_distances[:, :k]  # get **indices** of the topk images for each row\n",
        "  topk_distances = distances[np.arange(nq)[:, None], topk]  # use the indices to get the topk scores (magic slicing version)\n",
        "  # topk_scores = np.concatenate([scores[i, topk[i]] for i in range(nq)])  # get topk scores (comprehensible version)\n",
        "  return topk, topk_distances\n",
        "\n",
        "# define an helper function to view the results\n",
        "def show_images(urls, figsize=None):\n",
        "  n_images = len(urls)\n",
        "  fig, axes = plt.subplots(1, n_images, figsize=figsize)\n",
        "  for ax, url in zip(axes, urls):\n",
        "    image_np = np.asarray(Image.open(requests.get(url, stream=True).raw))\n",
        "    # image_np = unpad_image(image_np)\n",
        "    image_np = resize(image_np, (400, 300))\n",
        "    ax.set_ylabel(f'Query')\n",
        "    ax.imshow(image_np)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "  return fig"
      ],
      "metadata": {
        "id": "7EVUq-hceED6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iBqVHUHBrFK"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWsn2E8hlZOx"
      },
      "source": [
        "Let's inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXFZpKlslXzy"
      },
      "source": [
        "image_paths = Path('mirflickr5k').rglob('*.jpg')\n",
        "image_paths = sorted(image_paths)\n",
        "image_paths[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yODlPgBzmbNu"
      },
      "source": [
        "fig, axes = plt.subplots(5, 10, figsize=(20, 10))\n",
        "for ax, image_path in zip(axes.flatten(), image_paths):\n",
        "  image_np = plt.imread(image_path)\n",
        "  image_np = resize(image_np, (400, 300))\n",
        "  ax.imshow(image_np)\n",
        "  ax.axis('off')\n",
        "plt.subplots_adjust(wspace=0, hspace=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcNvw0RUC0xE"
      },
      "source": [
        "Consider a (potentially large) database of images and a set of query images.\n",
        "Our goal is to retrieve images from the database that are visually similar to the queries.\n",
        "\n",
        "Let's first select some images among which we will search. We will define our queries later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYVjijCXoGsL"
      },
      "source": [
        "ndb = 1000 # number of samples in the database to consider\n",
        "\n",
        "selected_image_paths = random.sample(image_paths, ndb)\n",
        "db_image_paths = selected_image_paths[:ndb]\n",
        "db_image_paths[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text to Image Retrieval\n",
        "\n",
        "We will try to retrieve images using natural texts as a query.\n",
        "\n",
        "We need:\n",
        "- a feature extractor for the _images_ (our _database_)\n",
        "- a feature extractor for the _texts_ (our _queries_)\n",
        "\n",
        "We will use the [CLIP](https://huggingface.co/docs/transformers/v4.19.2/en/model_doc/clip) deep neural network, which implements both the feature extractors.\n",
        "\n",
        "This model is trained\n",
        "- to extract **representations of images** (image features)\n",
        "- and **representations of short text sentences** (text features)\n",
        "- such that those representations **match** when the text describe the image content.\n",
        "\n",
        "Let's initialize CLIP."
      ],
      "metadata": {
        "id": "mGleUgjBbBvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "4J2rR9YYyJtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image and text representations\n",
        "\n",
        "Now we define some nice helper functions:\n",
        "- the `extract_features_from_images`, which extracts representations from the images;\n",
        "- the `extract_features_from_texts`, which extracts representations from the images.\n",
        "\n",
        "We hide the internals of these functions, as there are some unimportant details. Feel free to unhide it if you want to know more.\n",
        "\n",
        "However, run the following block before moving on!"
      ],
      "metadata": {
        "id": "X94EcR9EvBYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_images(images):\n",
        "  features = []\n",
        "\n",
        "  # we repeat the extraction for each image in the given list of images\n",
        "  for image in tqdm(images):\n",
        "\n",
        "    # open the image\n",
        "    image = Image.open(image)\n",
        "\n",
        "    # perform some preprocessing (scale, normalization) on the image\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # extract the features from the image using our CLIP deep neural network\n",
        "    with torch.no_grad():\n",
        "      feature = model.get_image_features(**inputs)\n",
        "\n",
        "    # save the features in a list that we will return\n",
        "    features.append(feature)\n",
        "\n",
        "  # do some post-processing on the features before returning them\n",
        "  features = torch.cat(features, dim=0)\n",
        "  features = F.normalize(features)\n",
        "\n",
        "  return features\n",
        "\n",
        "\n",
        "def extract_features_from_texts(texts):\n",
        "\n",
        "  # preprocess the words of the text\n",
        "  inputs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "  inputs = inputs.to(device)\n",
        "\n",
        "  # extract the features from the text using the CLIP deep neural network\n",
        "  with torch.no_grad():\n",
        "    features = model.get_text_features(**inputs)\n",
        "\n",
        "  # do some post-processing on the features before returning them\n",
        "  features = F.normalize(features)\n",
        "  return features"
      ],
      "metadata": {
        "id": "Jh6VLUziyQyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's go with feature extraction!\n",
        "\n",
        "Now, call the `extract_features_from_images` function to extract the image features from the whole dataset.\n",
        "\n",
        "Look at the dimensionality of the extracted features. For each of the image in the database, we have a 512-dimensional feature (for ResNet it was 2048)."
      ],
      "metadata": {
        "id": "9u99tZ38voc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_dataset_features = extract_features_from_images(db_image_paths)\n",
        "print(image_dataset_features.shape)"
      ],
      "metadata": {
        "id": "Kwg7Caw20ziH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define some textual queries and we extract their features using this `extract_features_from_texts` function.\n",
        "\n",
        "Note that the dimensionality of the textual features is again 512, as the image features! We can therefore compute the Euclidean distance between them."
      ],
      "metadata": {
        "id": "VFCjJRtfzVoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textual_queries = [\n",
        "                   'A person riding a bike',\n",
        "                   'A picture of a young child',\n",
        "                   'A view of some mountains',\n",
        "                   'A laptop'\n",
        "]\n",
        "\n",
        "textual_queries_features = extract_features_from_texts(textual_queries)\n",
        "print(textual_queries_features.shape)"
      ],
      "metadata": {
        "id": "qgGDXRKj4wgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The core of text-to-image similarity search\n",
        "\n",
        "Once we have feature vectors representations, we can search similar representations by comparing the features vectors instead of pixels.\n",
        "\n",
        "We will compare feature vectors using the Euclidean distance (as we did for the image-retrieval case! Nothing changed here)\n",
        "$$\n",
        "d = \\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \\dotso + (x_n-y_n)^2},\n",
        "$$\n",
        "where $\\{x_1, x_2, ... x_n\\}$ are the coordinates of the first feature and $\\{y_1, y_2, ... y_n\\}$ those of the second."
      ],
      "metadata": {
        "id": "dzXy45zzpa30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distances = euclidean_distance(textual_queries_features, image_dataset_features)"
      ],
      "metadata": {
        "id": "jczAwaQQpj6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can reuse the `k_nearest_neighbors` function already used in Part 1, as is, for searching image representations more similar to text representations!\n",
        "\n",
        "Therefore, remember what we are doing under the hood:\n",
        "1. We sort the distances from the smallest to the largest, for each query.\n",
        "2. We take the first $k$ features as a result, again for each query."
      ],
      "metadata": {
        "id": "T1vlUhrtznbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "topk, topk_distances = k_nearest_neighbors(distances, k)\n",
        "print(topk_distances)"
      ],
      "metadata": {
        "id": "lHuI9syQ5Yf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Look at the results!\n",
        "\n",
        "Let's finally view the results."
      ],
      "metadata": {
        "id": "ZhlVMwtn0MSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nq, ndb = distances.shape\n",
        "\n",
        "# show topk similar\n",
        "fig, axes = plt.subplots(k, nq, figsize=(18, 4*k))\n",
        "for j in range(k):\n",
        "  axes[j, 0].set_ylabel(f'Rank #{j}')\n",
        "  for i in range(nq):\n",
        "    if j == 0:\n",
        "      axes[0, i].set_title(textual_queries[i])\n",
        "    image_np = plt.imread(db_image_paths[topk[i, j]])\n",
        "    # image_np = unpad_image(image_np)\n",
        "    image_np = resize(image_np, (400, 300))\n",
        "    axes[j, i].imshow(image_np)\n",
        "    axes[j, i].xaxis.set_label_position('top')\n",
        "    axes[j, i].set_xlabel('dist = {:.2f}'.format(topk_distances[i, j]))\n",
        "    axes[j, i].set_xticks([])\n",
        "    axes[j, i].set_yticks([])"
      ],
      "metadata": {
        "id": "-UggojJs5zSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try yourself\n",
        "You could try the following things:\n",
        "- change the number $k$ of neirest neighbors to retrieve for each query;\n",
        "- try to write other textual queries to understand what are the nice properties and the limitations of this approach. For example:\n",
        " - try queries with colors (e.g., \"There is a _red_ thing on top of the table\")\n",
        " - try queries with spatial indications (\"a person _to the right of_ a car\"))\n",
        "- try to change the number of images to retrieve (to 1000 to 3000 for example, to show if the results change). _Warning: feature extraction will be very slow :(_"
      ],
      "metadata": {
        "id": "kfFPVQ6GdTSE"
      }
    }
  ]
}