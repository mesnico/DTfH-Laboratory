{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_similarity_search.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "scFQtUonWXjR",
        "X94EcR9EvBYW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesnico/DTfH-Laboratory/blob/main/image_similarity_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4MK2n10BGmc"
      },
      "source": [
        "# Laboratory on Image Representations for Similarity Search\n",
        "-----------------\n",
        "\n",
        "You'll learn to:\n",
        "\n",
        "*   Represent images using features extracted from pretrained deep neural networks.\n",
        "*   Search images by visual similarity.\n",
        "*   Search images using textual descriptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0IgnESPfmK1"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "First of all, we need to download and unzip the image dataset, and install some Python dependencies.\n",
        "\n",
        "We will use  [MIRFlickr25k](https://press.liacs.nl/mirflickr/mirdownload.html), which contains photographs downloaded from the popular Flickr website.\n",
        "\n",
        "Run the following for getting the environment ready! This could take some minutes...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6_FRKubksmR"
      },
      "source": [
        "!wget http://press.liacs.nl/mirflickr/mirflickr25k.v3b/mirflickr25k.zip\n",
        "!unzip -n mirflickr25k.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we install some python dependency and we import the needed python packages"
      ],
      "metadata": {
        "id": "dah8Ta-nRIWE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxr0appUBuAh"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import transformers\n",
        "from PIL import Image\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "import requests\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "random.seed(42)\n",
        "\n",
        "euclidean_distance = torch.cdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the function for computing the k-NN from the distances\n",
        "def k_nearest_neighbors(distances, k=5):\n",
        "  nq, ndb = distances.shape\n",
        "\n",
        "  sorted_distances = distances.argsort(axis=1)  # sort the scores ascending, for each query\n",
        "  topk = sorted_distances[:, :k]  # get **indices** of the topk images for each row\n",
        "  topk_distances = distances[np.arange(nq)[:, None], topk]  # use the indices to get the topk scores (magic slicing version)\n",
        "  # topk_scores = np.concatenate([scores[i, topk[i]] for i in range(nq)])  # get topk scores (comprehensible version)\n",
        "  return topk, topk_distances\n",
        "\n",
        "# define an helper function to view the results\n",
        "def show_images(urls, figsize=None):\n",
        "  n_images = len(urls)\n",
        "  fig, axes = plt.subplots(1, n_images, figsize=figsize)\n",
        "  for ax, url in zip(axes, urls):\n",
        "    image_np = np.asarray(Image.open(requests.get(url, stream=True).raw))\n",
        "    # image_np = unpad_image(image_np)\n",
        "    image_np = resize(image_np, (400, 300))\n",
        "    ax.set_ylabel(f'Query')\n",
        "    ax.imshow(image_np)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "  \n",
        "  return fig"
      ],
      "metadata": {
        "id": "7EVUq-hceED6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iBqVHUHBrFK"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWsn2E8hlZOx"
      },
      "source": [
        "Let's inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXFZpKlslXzy"
      },
      "source": [
        "image_paths = Path('mirflickr').rglob('*.jpg')\n",
        "image_paths = sorted(image_paths)\n",
        "image_paths[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yODlPgBzmbNu"
      },
      "source": [
        "fig, axes = plt.subplots(5, 10, figsize=(20, 10))\n",
        "for ax, image_path in zip(axes.flatten(), image_paths):\n",
        "  image_np = plt.imread(image_path)\n",
        "  image_np = resize(image_np, (400, 300))\n",
        "  ax.imshow(image_np)\n",
        "  ax.axis('off')\n",
        "plt.subplots_adjust(wspace=0, hspace=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcNvw0RUC0xE"
      },
      "source": [
        "## Part 1 - Image Retrieval by Visual Similarity\n",
        "\n",
        "Consider a (potentially large) database of images and a set of query images.\n",
        "Our goal is to retrieve images from the database that are visually similar to the queries.\n",
        "\n",
        "Let's first select some images among which we will search. We will define our queries later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYVjijCXoGsL"
      },
      "source": [
        "ndb = 1000 # number of samples in the database to consider\n",
        "\n",
        "selected_image_paths = random.sample(image_paths, ndb)\n",
        "db_image_paths = selected_image_paths[:ndb]\n",
        "db_image_paths[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTwfGoVi1rAI"
      },
      "source": [
        "### Prepare the deep neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwHP-2jr1tWw"
      },
      "source": [
        "We will use a pretrained model to extract features from images and use them as compact image representations.\n",
        "The choice of the model is essential to get good representations for your target task (e.g. a model trained on classifying dog breeds will have a hard time creating good representations for images of flowers).\n",
        "\n",
        "In this laboratory, we will use the models from the [HuggingFace](https://huggingface.co/) repository. It collects a bunch of different models pre-trained on [ImageNet-1k](https://www.image-net.org/download.php), the dataset on which deep networks for image classification are trained.\n",
        "\n",
        "In particular, we will use one of the most recent state-of-the-art deep neural networks as image feature extractor: [ResNet](https://huggingface.co/docs/transformers/v4.19.2/en/model_doc/resnet).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL4YG1xKA_EO"
      },
      "source": [
        "# a model pretrained on ImageNet-21k\n",
        "model = transformers.ResNetModel.from_pretrained(\"microsoft/resnet-50\")\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "preprocess = transformers.AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soJ1TXPViPDd"
      },
      "source": [
        "`preprocess` is a module that performs some preprocessing on the image before feeding it to the deep neural network (scale, normalization).\n",
        "\n",
        "`model` is the actual deep neural network (our ResNet) that we will use for extracting the features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction function\n",
        "\n",
        "Let's prepare a function `extract_features` that extracts the features from a set of images given as input, using the pre-defined deep neural network.\n",
        "\n",
        "We hide the internals of this function, as there are some unimportant details. Feel free to unhide it if you want to know more.\n",
        "\n",
        "However, run it before moving on!"
      ],
      "metadata": {
        "id": "scFQtUonWXjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(images, extraction_layer=-1, normalize=True):\n",
        "  features = []\n",
        "\n",
        "  # we repeat the extraction for each image in the given list of images\n",
        "  for image in tqdm(images):\n",
        "\n",
        "    # open the image\n",
        "    image = Image.open(image)\n",
        "\n",
        "    # perform some preprocessing (scale, normalization) on the image\n",
        "    inputs = preprocess(image, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # extract the features from the image using our deep neural network\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**inputs, output_hidden_states=True)\n",
        "    \n",
        "    # take the representations from a specific layer of the deep neural network (by default, from the last)\n",
        "    feature = outputs.hidden_states[extraction_layer]\n",
        "\n",
        "    # save the features in a list that we will return\n",
        "    features.append(feature.mean(dim=[2,3]).cpu())\n",
        "\n",
        "  # do some post-processing on the features before returning them\n",
        "  features = torch.cat(features, dim=0)\n",
        "  if normalize:\n",
        "    features = F.normalize(features)\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "uZtE0eM8eJyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNuo5_ufolP1"
      },
      "source": [
        "### Let's go with feature extraction!\n",
        "\n",
        "Now, we can call this nice `extract_features` function on the list of images that we have chosen from the dataset.\n",
        "\n",
        "Notice that the output shape is a table having a number of rows equal to the number of images, and a number of column equal to the number of dimensions of our feature. In practice, for each image in the dataset we have a 2048-dimensional feature.\n",
        "\n",
        "The `extraction_layer` parameter controls from which layer we extract the features.\n",
        "- -1 means the last layer\n",
        "- -2 means the second-last layer\n",
        "- ...\n",
        "\n",
        "For now, we use the features from the last layer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_features = extract_features(db_image_paths, extraction_layer=-1)\n",
        "print(dataset_features.shape)"
      ],
      "metadata": {
        "id": "d1c1UQ4lA8mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CiRDyuj7a68"
      },
      "source": [
        "### Choose some query images\n",
        "\n",
        "First of all, let's select some query images for which we want to retrieve similar images in our MIRFlickr25k database.\n",
        "\n",
        "We will use some image taken from the web as queries (you can experiment with any image URL you find on the web!)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries_urls = [\n",
        "        'http://farm4.staticflickr.com/3472/3208721055_d4d9ba651b_z.jpg',\n",
        "        'http://farm6.staticflickr.com/5267/5635183343_5aff7a08af_z.jpg',\n",
        "        'https://farm7.staticflickr.com/6168/6148670625_1caaf41c72_z.jpg',\n",
        "        'http://farm9.staticflickr.com/8233/8471600250_d2ab877363_z.jpg'\n",
        "]\n",
        "\n",
        "# let's display them\n",
        "fig, axes = plt.subplots(1, len(queries_urls), figsize=(10, 5))\n",
        "for ax, url in zip(axes.flatten(), queries_urls):\n",
        "  image_np = np.asarray(Image.open(requests.get(url, stream=True).raw))\n",
        "  image_np = resize(image_np, (400, 300))\n",
        "  ax.imshow(image_np)\n",
        "  ax.axis('off')\n",
        "plt.subplots_adjust(wspace=.1, hspace=0)"
      ],
      "metadata": {
        "id": "dH6Bs0RehPks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can produce the features for the queries that we have just chosen.\n",
        "\n",
        "We do this exactly as before: we call our nice function `extract_features`, which uses our nice deep neural network for obtaining features from the query images."
      ],
      "metadata": {
        "id": "RE9vwLbHnq8g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faskiG7Q2laH"
      },
      "source": [
        "# download the query images from the web\n",
        "queries = [requests.get(url, stream=True).raw for url in queries_urls]\n",
        "\n",
        "# extract features of queries\n",
        "queries_features = extract_features(queries, extraction_layer=-1)\n",
        "print(queries_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwqyNujbsK_c"
      },
      "source": [
        "### The core of image similarity search\n",
        "\n",
        "Once we have feature vectors representations, we can search similar representations by comparing the features vectors instead of pixels.\n",
        "\n",
        "We will compare feature vectors using the Euclidean distance\n",
        "$$\n",
        "d = \\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \\dotso + (x_n-y_n)^2},\n",
        "$$\n",
        "where $\\{x_1, x_2, ... x_n\\}$ are the coordinates of the first feature and $\\{y_1, y_2, ... y_n\\}$ those of the second.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq_fb4bZ5sNA"
      },
      "source": [
        "distances = euclidean_distance(queries_features, dataset_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the distances, we can compute the k-nearest-neighbors for each query!\n",
        "The function `k_nearest_neighbors` performs exactly this.\n",
        "\n",
        "1. It sorts the distances from the smallest to the largest, for each query.\n",
        "2. It takes the first $k$ features as a result, again for each query."
      ],
      "metadata": {
        "id": "1batEVnBZbEz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1aBx5368GWu"
      },
      "source": [
        "k = 5 # how many neighbors for each query\n",
        "topk, topk_distances = k_nearest_neighbors(distances, k)\n",
        "print(topk_distances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psH4XCeIuXgx"
      },
      "source": [
        "### Look at the results!\n",
        "\n",
        "Let's finally view the results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nq, ndb = distances.shape\n",
        "\n",
        "# show queries\n",
        "fig = show_images(queries_urls, figsize=(3*nq, 10))\n",
        "\n",
        "# show topk similar \n",
        "fig, axes = plt.subplots(k, nq, figsize=(fig.get_figwidth(), 4*k))\n",
        "for j in range(k):\n",
        "  axes[j, 0].set_ylabel(f'Rank #{j}')\n",
        "  for i in range(nq):\n",
        "    image_np = plt.imread(db_image_paths[topk[i, j]])\n",
        "    image_np = resize(image_np, (400, 300))\n",
        "    axes[j, i].imshow(image_np)\n",
        "    axes[j, i].set_title('dist = {:.2f}'.format(topk_distances[i, j]))\n",
        "    axes[j, i].set_xticks([])\n",
        "    axes[j, i].set_yticks([])"
      ],
      "metadata": {
        "id": "CTlAeb7gqoZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try yourself\n",
        "You could try the following things:\n",
        "- change the number $k$ of neirest neighbors to retrieve for each query;\n",
        "- find other images on the web as queries, e.g. using Google Images;\n",
        "- change the layer of the deep neural network from which the features are extracted (e.g.: from `extraction_layer=-1` to `extraction_layer=-2`) to see what changes.\n",
        "- try to change the number of images to retrieve (to 1000 to 3000 for example, to show if the results change). _Warning: feature extraction will be very slow :(_"
      ],
      "metadata": {
        "id": "DczzkuZkawv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Text to Image Retrieval\n",
        "\n",
        "In this second part, instead, we will try to retrieve images using natural texts as a query.\n",
        "\n",
        "We need:\n",
        "- a feature extractor for the _images_ (our _database_)\n",
        "- a feature extractor for the _texts_ (our _queries_)\n",
        "\n",
        "We will use the [CLIP](https://huggingface.co/docs/transformers/v4.19.2/en/model_doc/clip) deep neural network, which implements both the feature extractors.\n",
        "\n",
        "Let's initialize CLIP."
      ],
      "metadata": {
        "id": "mGleUgjBbBvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "4J2rR9YYyJtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image and text representations\n",
        "\n",
        "Now we define some nice helper functions:\n",
        "- the `extract_features_from_images`, which extracts representations from the images;\n",
        "- the `extract_features_from_texts`, which extracts representations from the images.\n",
        "\n",
        "We hide the internals of these functions, as there are some unimportant details. Feel free to unhide it if you want to know more.\n",
        "\n",
        "However, run the following block before moving on!"
      ],
      "metadata": {
        "id": "X94EcR9EvBYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_images(images):\n",
        "  features = []\n",
        "\n",
        "  # we repeat the extraction for each image in the given list of images\n",
        "  for image in tqdm(images):\n",
        "\n",
        "    # open the image\n",
        "    image = Image.open(image)\n",
        "\n",
        "    # perform some preprocessing (scale, normalization) on the image\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # extract the features from the image using our CLIP deep neural network\n",
        "    with torch.no_grad():\n",
        "      feature = model.get_image_features(**inputs)\n",
        "\n",
        "    # save the features in a list that we will return\n",
        "    features.append(feature)\n",
        "\n",
        "  # do some post-processing on the features before returning them\n",
        "  features = torch.cat(features, dim=0)\n",
        "  features = F.normalize(features)\n",
        "\n",
        "  return features\n",
        "  \n",
        "\n",
        "def extract_features_from_texts(texts):\n",
        "\n",
        "  # preprocess the words of the text\n",
        "  inputs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "  inputs = inputs.to(device)\n",
        "\n",
        "  # extract the features from the text using the CLIP deep neural network\n",
        "  with torch.no_grad():\n",
        "    features = model.get_text_features(**inputs)\n",
        "\n",
        "  # do some post-processing on the features before returning them\n",
        "  features = F.normalize(features)\n",
        "  return features"
      ],
      "metadata": {
        "id": "Jh6VLUziyQyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Let's go with feature extraction!\n",
        "\n",
        "Now, call the `extract_features_from_images` function to extract the image features from the whole dataset.\n",
        "\n",
        "Look at the dimensionality of the extracted features. For each of the image in the database, we have a 512-dimensional feature (for ResNet it was 2048)."
      ],
      "metadata": {
        "id": "9u99tZ38voc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_dataset_features = extract_features_from_images(db_image_paths)\n",
        "print(image_dataset_features.shape)"
      ],
      "metadata": {
        "id": "Kwg7Caw20ziH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define some textual queries and we extract their features using this `extract_features_from_texts` function.\n",
        "\n",
        "Note that the dimensionality of the textual features is again 512, as the image features! We can therefore compute the Euclidean distance between them."
      ],
      "metadata": {
        "id": "VFCjJRtfzVoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textual_queries = [\n",
        "                   'A person riding a bike',\n",
        "                   'A picture of an old man',\n",
        "                   'A view of some mountains',\n",
        "                   'There are some people in a room with a laptop'\n",
        "]\n",
        "\n",
        "textual_queries_features = extract_features_from_texts(textual_queries)\n",
        "print(textual_queries_features.shape)"
      ],
      "metadata": {
        "id": "qgGDXRKj4wgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The core of text-to-image similarity search\n",
        "\n",
        "As in Part 1, we compute the Euclidean distance between the queries (that this time come from texts!) and the images in the dataset."
      ],
      "metadata": {
        "id": "dzXy45zzpa30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distances = euclidean_distance(textual_queries_features, image_dataset_features)"
      ],
      "metadata": {
        "id": "jczAwaQQpj6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can reuse the `k_nearest_neighbors` function already used in Part 1, as is, for searching image representations more similar to text representations!\n",
        "\n",
        "Therefore, remember what we are doing under the hood:\n",
        "1. We sort the distances from the smallest to the largest, for each query.\n",
        "2. We take the first $k$ features as a result, again for each query. "
      ],
      "metadata": {
        "id": "T1vlUhrtznbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "topk, topk_distances = k_nearest_neighbors(distances, k)"
      ],
      "metadata": {
        "id": "lHuI9syQ5Yf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Look at the results!\n",
        "\n",
        "Let's finally view the results."
      ],
      "metadata": {
        "id": "ZhlVMwtn0MSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nq, ndb = distances.shape\n",
        "\n",
        "# show topk similar\n",
        "fig, axes = plt.subplots(k, nq, figsize=(18, 4*k))\n",
        "for j in range(k):\n",
        "  axes[j, 0].set_ylabel(f'Rank #{j}')\n",
        "  for i in range(nq):\n",
        "    if j == 0:\n",
        "      axes[0, i].set_title(textual_queries[i])\n",
        "    image_np = plt.imread(db_image_paths[topk[i, j]])\n",
        "    # image_np = unpad_image(image_np)\n",
        "    image_np = resize(image_np, (400, 300))\n",
        "    axes[j, i].imshow(image_np)\n",
        "    axes[j, i].xaxis.set_label_position('top')\n",
        "    axes[j, i].set_xlabel('dist = {:.2f}'.format(topk_distances[i, j]))\n",
        "    axes[j, i].set_xticks([])\n",
        "    axes[j, i].set_yticks([])"
      ],
      "metadata": {
        "id": "-UggojJs5zSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try yourself\n",
        "You could try the following things:\n",
        "- change the number $k$ of neirest neighbors to retrieve for each query;\n",
        "- try to write other textual queries to understand what are the nice properties and the limitations of this approach. For example:\n",
        " - try queries with colors (e.g., \"There is a _red_ thing on top of the table\")\n",
        " - try queries with spatial indications (\"a person _to the right of_ a car\"))\n",
        "- try to change the number of images to retrieve (to 1000 to 3000 for example, to show if the results change). _Warning: feature extraction will be very slow :(_"
      ],
      "metadata": {
        "id": "kfFPVQ6GdTSE"
      }
    }
  ]
}